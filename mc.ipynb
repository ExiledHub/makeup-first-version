{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 1. Imports\n---\n<p>\nNumpy: Math related stuff but mainly for linear algebra.\n</p>\n<p>\nPandas: Importing and processing data.\n</p>\n<p>\nRandom: Helps us randomize an array of items into a normal distrition.\n</p>\n<p>\nMatplotlib: Plots the images and displays it nicely.\n</p>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nfrom PIL import Image\n\nimport pathlib\nimport imageio\nimport PIL\nimport h5py\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, datasets\nfrom keras.models import model_from_json\nfrom keras import regularizers\n\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nrandom.seed(69)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Organization\n---\nNow that we have defined our imports, lets go ahead and get our data imported and organized."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters\nstandard_width = 150\nstandard_height = 150\nimage_channels = 3\n\ntraining_data = []\nprediction_data = []\n\n# Get raw images from the Makeup collection and sort them checking they are unique.\nraw_images = pathlib.Path('/kaggle/input/make-up-vs-no-make-up/data/data/makeup').glob('*.jpeg')\nsorted_images_makeup = sorted([ image for image in raw_images])\n\n# Get raw images from the No Makeup collection and sort them checking they are unique.\nraw_images = pathlib.Path('/kaggle/input/make-up-vs-no-make-up/data/data/no_makeup').glob('*.jpeg')\nsorted_images_noMakeup = sorted([ image for image in raw_images])\n\nnumber_of_samples = len(sorted_images_noMakeup) + len(sorted_images_makeup)\n\nprint(len(sorted_images_makeup))\nprint(len(sorted_images_noMakeup))\nprint(number_of_samples)\n\n# We need to build a 4D tensor that looks something like this\n# [number_of_samples,standard_width,standard_height,image_channels]\n\n# Path stores the relative path to the image while the makeup stores a boolean.\n# Think about it as the input and the output of the CNN.\nfor path in sorted_images_makeup:\n    im = Image.open(path) # Read Image\n    im = im.resize((standard_width,standard_height)) # Reshape it to a standard size (w,h,3)\n    image = np.array(im) # Convert into numpy array.\n    training_data.append(image) # Push it into the training data.\n    prediction_data.append(True)\n\n# Path stores the relative path to the image while the makeup stores a boolean.\n# Think about it as the input and the output of the CNN.\nfor path in sorted_images_noMakeup:\n    im = Image.open(path) # Read Image\n    im = im.resize((standard_width,standard_height)) # Reshape it to a standard size (w,h,3)\n    image = np.array(im) # Convert into numpy array.\n    training_data.append(image) # Push it into the training data.\n    prediction_data.append(False)\n    \ntraining_data = np.asarray(training_data)\n\nrng_state = np.random.get_state()\nnp.random.shuffle(training_data)\nnp.random.set_state(rng_state)\nnp.random.shuffle(prediction_data)\nrandom.shuffle(training_data)\n\nprint('We have', len(training_data), 'training samples')\nprint(training_data)\nprint(training_data.shape)\n#print('We are going to analyse',len(sorted_images),\"images\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Early Image Visualization\n---\nNow that we have all the paths for our images and lables for each one of them, we can check using matplotlib by ploting the images in real-time. Lets check #69 because why not."},{"metadata":{"trusted":true},"cell_type":"code","source":"image = training_data[69]\nplt.imshow(image)\nplt.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. ConvNet\n---\nThe Convolutional Neural Network will take an image as an input and return us if it has make-up or not. We have data in this moment made out of images and lables, so the input and the output of the network. We now need trainging and test data out of this data. For this, we need to randomly select makeup and no-makeup images for both collections. Also, normalization in this case is not absolutly needed, but we will apply it anyways just to make sure we don't have exploding gradients later.\n\nOur architecture will consist of a Conv (2x2) + ReLu, then a Conv (4x4) + ReLu and finally a max pooling layer. We will repeat that 5 times, then we will flatten the result and feed it into 5 fully connected layers, ending on a Softmax. This means 7 Artificial Layers including the result layer. \n\nWe have a total of 1506 images, we are going to use 500 for testing and 1006 for training. Lets start by getting that organized."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_size = 500\ntraining_images = training_data[:- test_size,:,:,:]\ntraining_labels = prediction_data[:- test_size]\ntest_images = training_data[- test_size:,:,:,:]\ntest_labels = prediction_data[- test_size:]\n\nprint('Data size',len(training_data))\nprint('Training images',len(training_images))\nprint('Training labels', len(training_labels))\nprint('Test images',len(test_images))\nprint('Test labels',len(test_labels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_width = 150\nimage_height = 150\nimage_channels = 3\n\n#1 conv\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(16, (2,2), padding=\"same\", activation='relu', \n                        kernel_regularizer=regularizers.l2(0.01),\n                        activity_regularizer=regularizers.l2(0.01),\n                        input_shape=(image_width,image_height,image_channels)))\nmodel.add(layers.Conv2D(32, (4,4), padding=\"same\", activation='relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\n\n#2 conv\nmodel.add(layers.Conv2D(32, (2,2), padding=\"same\", activation='relu'))\nmodel.add(layers.Conv2D(64, (4,4), padding=\"same\", activation='relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\n\n#3 conv\nmodel.add(layers.Conv2D(64, (2,2), padding=\"same\", activation='relu'))\nmodel.add(layers.Conv2D(128, (4,4), padding=\"same\", activation='relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\n\n#4 conv\nmodel.add(layers.Conv2D(128, (2,2), padding=\"same\", activation='relu'))\nmodel.add(layers.Conv2D(256, (4,4), padding=\"same\", activation='relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\n\n#5 conv\nmodel.add(layers.Conv2D(256, (2,2), padding=\"same\", activation='relu'))\nmodel.add(layers.Conv2D(512, (4,4), padding=\"same\", activation='relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\n\n# Flatten\nmodel.add(layers.Flatten())\n\n#1 dense\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dropout(0.05))\n\n#2 dense\nmodel.add(layers.Dense(64, activation='relu'))\n\n#3 dense\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dropout(0.05))\n\n#4 dense\nmodel.add(layers.Dense(64, activation='relu'))\n\n#5 dense\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dropout(0.05))\n\n# Softmax\nmodel.add(layers.Dense(2, activation='softmax'))\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Training\nNow that we have build a model and organized the data into a nice randomized batched, lets get started with training the beast! We need to define a few things first, like what type of optimizer we are going to use and the loss function. We are going to use Adam optimization and crossvalidation in this case."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_model = True\n\nif train_model:\n    model.compile(optimizer='adam',\n             loss='sparse_categorical_crossentropy',\n             metrics=['accuracy'])\n\n    print('There should be', len(training_images), '=', len(training_labels))\n    print('There should be', len(test_images), '=', len(test_labels))\n    print('training_images',len(training_images))\n    print('training_lables',len(training_labels))\n\n    history = model.fit(training_images, training_labels, batch_size=64, epochs=10)\n\n    print(print('\\nhistory dict:', history.history))\n\n    test_results = model.evaluate(test_images, test_labels, batch_size=128)\n\n    print('test loss, test acc:', test_results)\n\n    print('\\n# Generate predictions for 3 samples')\n    predictions = model.predict(test_images[:3])\n    print('predictions shape:', predictions.shape)\nelse:\n    # load json and create model\n    json_file = open('model.json', 'r')\n    loaded_model_json = json_file.read()\n    json_file.close()\n    model = model_from_json(loaded_model_json)\n    # load weights into new model\n    model.load_weights(\"model.h5\")\n    print(\"Loaded model from disk\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have trained our model, time for us to save it to disk, as we might want to use it later without having to train in all over again. For that we are going to use Keras and h5py in order to make a JSON representation of our model and store it in disk."},{"metadata":{"trusted":true},"cell_type":"code","source":"# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, lets try to predict some cases. In this case, you will need to do some manual uploading of cases and run them yourself."},{"metadata":{"trusted":true},"cell_type":"code","source":"path = \"/kaggle/input/make-up-vs-no-make-up/data/data/makeup/makeup1036.jpeg\"\n# Get raw images from the Makeup collection and sort them checking they are unique.\nim = Image.open(path) # Read Image\nim = im.resize((standard_width,standard_height)) # Reshape it to a standard size (w,h,3)\nimage = np.array(im) # Convert into numpy array.\ninput_state = []\ninput_state.append(image)\n\n# Generate predictions (probabilities -- the output of the last layer)\n# on new data using `predict`\nprint('\\n# Generate predictions for 3 samples')\npredictions = model.predict(image_state)\nprint('predictions shape:', predictions.shape)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}